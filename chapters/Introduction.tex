Programming languages are used every day by millions of engineers as part of their daily routine.
A programming language is used to tell a computer what to do. When
somebody wants a computer to do something, it needs to write a program using a programming language. Then
a compiler needs to translate it into machine code for the computer to execute it.

To design a programming language it's important to understand the theory behind a compiler and learn about all the different
steps involved to make a computer understand and execute a programming language.

\section{Objectives}

The main objective of this thesis is to design an experimental programming language and implement an interpreter to execute
any program written with it. The different challenges that are faced during the design and implementation process are also explained and
solved in their respective chapters. The experimental language built in this thesis is called \textbf{Nuua}.

The objective can be partitioned into the following points.

\begin{itemize}
    \item Learn all the steps involved in a common compiler implementation and reproduce them acording to the project needs.
    \item Design the Nuua Programming Language. This grammar must be simple, elegant and yet it needs to follow the most
        common programming language's specifications to avoid confusion when learning it, allowing either mature programmers or newcomers to pick up the grammar quickly and efficiently.
    \item Choose an efficient programming language to build the compiler with. Among other options, the languages that satisfy the previous
    statement are low-level programming languages like C, C++, D, Rust, Go, Crystal, Zig or Nim.
    \item Define a scalable software system to allow the compiler to grow efficiently. The software system that is chosen will be very important towards the scalability of the project.
    \item Build a compiler for the Nuua programming language and a very simple standard library.
\end{itemize}

\section{Preliminary overview}

This section details the most important aspects found in a compiler.
This preliminary overview won't deal with details and only explains the basics to understand the whole system without deep knowledge.
Further chapters contain expanded information respective to some of the details mentioned here.

\subsection{Language grammar}

Context-free grammar is a notation used to specify the syntax of a programming language. Following the syntax definition explained
in \autocite[Section~2.2]{compilers} a context-free grammar consists of four components:

\begin{enumerate}
    \item A group of terminal symbols also known as tokens. In a programming language tokens may be literal symbols like '+', '*' or numbers and identifiers.
    \item A group of non-terminals that can be reduced to terminals based on the production rules.
    \item A group of production rules that consists of a non-terminal on the left side and a sequence of terminals and/or non-terminals on the right side.
    \item A non-terminal start symbol.
\end{enumerate}

This thesis will use the \emph{extended Backus-Naur form} also known as \emph{EBNF} to express the context-free grammar representation of Nuua.
EBNF is often used in different ways due to the big ammount of variants that exists. To clarify the EBNF syntax that this thesis is going to use, the
following table will be used to specify the production rules. More information may be found in the EBNF grammar article \autocite{EBNF_grammar}.

\begin{table}[H]
    \centering
    \begin{tabular}{ l p{10cm} }
        \textbf{Symbol} & \textbf{Definition} \\
        \texttt{:} & Used to define a production rule. \\
        \texttt{\textit{space}} & Used to concatenate patterns (space separated). \\
        \texttt{A|B} & Used to define a union of A and B. \\
        \texttt{A+} & Used to define a one or more pattern of A. \\
        \texttt{A*} & Used to define a zero or more pattern. \\
        \texttt{A?} & Used to define an optional pattern. \\
        \texttt{(A)} & Used to group a pattern. \\
        \texttt{"T"} or \texttt{'T'} & Used to define a terminal symbol. \\
        \texttt{;} & Used to terminate a given production rule. \\
    \end{tabular}
    \caption{EBNF syntax}
    \label{fig:ebnf_syntax}
\end{table}

As a simple example, to define a language that consists in a single integer, the following EBNF grammar could be used:\\

\texttt{integer\\\tab: ("-" | "+")? digit+\\\tab;}\\
\texttt{digit\\\tab: "0"|"1"|"2"|"3"|"4"|"5"|"6"|"7"|"8"|"9"\\\tab;}\\

This variation is often used by many parser generators since it introduces a more visible and versatile approach to
write the language grammar.

\subsection{Compilers}

The job of a compiler is to take an input language and translate it into another as showin in \autoref{fig:compiler_overview}.
Often, the compiler term is used to express a translation to a much different level of abstraction, that usually means
that the input is written in a high-level language and further translated into another low-level language.
However, there are multiple types of compilers, and each one is used to implement different translation tasks.
Nuua's system architecture built in this thesis, as described in \autoref{sec:sys_arch}, is based on a interpreter architecture, wich consists of a
compiler that translates the Nuua language into an intermediate representation known as \emph{bytecode} and further interpreted by a virtual machine.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        [node distance=1cm]

        % Nodes of the layered system
        \node[block] (input) {Input language};
        \node[block,right=of input] (compiler) {Compiler};
        \node[block,right=of compiler] (output) {Output language};

        % Lines
        \draw[line] (input) -- (compiler);
        \draw[line] (compiler) -- (output);

    \end{tikzpicture}

    % Caption and Label
    \caption{Compiler overview}
    \label{fig:compiler_overview}
\end{figure}

\subsection{Phases of a compiler}

A compiler can also be decoupled into different parts. Each part does a very different job but they are all connected to each other.
In a typical compiler architecture, we may find all the different phases described in \autoref{fig:compiler_phases}.

Those phases are often found to be different depending on the implementation of the language. However, it's important to note what they do,
since they are often implemented in one way or another. More implementation details are explained in their respective chapters but in this
section, a small introduction to each phase is needed to understand the Nuua's system.

\begin{figure}[p]
    \centering
    \begin{tikzpicture}
        [node distance=1cm]

        \node (input) {Input};
        \node[bigger_block,below=of input] (lexical) {Lexical analysis (Lexer or scanner)};
        \node[bigger_block,below=of lexical] (syntax) {Syntax analysis (Parser)};
        \node[bigger_block,below=of syntax] (semantic) {Semantic analysis};
        \node[bigger_block,below=of semantic] (inter) {Intermediate code};
        \node[bigger_block,below=of inter] (opt) {Optimization};
        \node[bigger_block,below=of opt] (codegen) {Code generation};
        \node[below=of codegen] (output) {Output};

        \draw[line] (input) -- (lexical);
        \draw[line] (lexical) -- (syntax);
        \draw[line] (syntax) -- (semantic);
        \draw[line] (semantic) -- (inter);
        \draw[line] (inter) -- (opt);
        \draw[line] (opt) -- (codegen);
        \draw[line] (codegen) -- (output);

        % The right specifiers
        \draw[tuborg, decoration={brace}] let \p1=(lexical.north), \p2=(semantic.south) in
            ($(3.5, \y1)$) -- ($(3.5, \y2)$) node[tubnode] {Front end};
        \draw[tuborg, decoration={brace}] let \p1=(inter.north), \p2=(opt.south) in
            ($(3.5, \y1)$) -- ($(3.5, \y2)$) node[tubnode] {Middle end};
        \draw[tuborg, decoration={brace}] let \p1=(codegen.north), \p2=(codegen.south) in
            ($(3.5, \y1)$) -- ($(3.5, \y2)$) node[tubnode] {Back end};
    \end{tikzpicture}

    % Caption and Label
    \caption{Common compiler phases}
    \label{fig:compiler_phases}
\end{figure}

\begin{itemize}
    \item \textbf{Lexical analysis}: In this phase, the input source is transformed from a character string into a token list, this is also called
        tokenization. Tokens are known and have certain attributes. For example, some tokens might include integers, symbols (+, -, *, etc.) or identifiers among others. Lexemes are also evaluated as individual tokens, making a single token for identifiers matched as language keywords
        (like 'if', 'while', etc.). This phase is also called a lexer or scanner.
    \item \textbf{Syntax analysis}: In this phase, the implementation may vary among compilers, some of them work close to the lexical analysis since they
        can work together. However, its purpose is to perform operations to the token list to parse the input and create an Abstract Syntax Tree
        (AST\footnote{\href{https://en.wikipedia.org/wiki/Abstract_syntax_tree}{Abstract Syntax Tree (Wikipedia)}}). An AST is already a structural representation that represents your input source. This stage determines if it is a valid program based on the language grammar and the specified rules. There are also scanner-less parsers that takes the lexical analysis and the syntax analysis into a single step. It is harder to understand and build but it often leads to faster parsing and less memory usage.
    \item \textbf{Semantic analysis}: This phase analyzes the AST and creates a symbol table while analyzes the input source with things like type checking or
        variable declarations, if some operations can be performed (for example adding a number with a string). A symbol table is a structure used by further phases to see information attached to specific source code parts. For example, it can store information about a variable (if it's global, exported, etc.).
    \item \textbf{Intermidiate code}: An intermediate representation (IR) can be avoided but it's often used to have a platform independent optimizer. Usually
        the code generation targets a specific architecture and would require different optimizers depending on each architecture. However, by having an IR it's possible to have a single optimizer. A much used IR is Three Adress Code (TAC\footnote{\href{https://en.wikipedia.org/wiki/Three-address_code}{Three Address Code (Wikipedia)}})
        that can be organized in quadruples or triples.
    \item \textbf{Optimization}: This optimization is often performed on the IR and performs different tasks to allow a faster and smaller output. For example, it may
        remove dead code, perform loop optimizations, etc.
    \item \textbf{Code generation}: This is where the real translation takes place, it translates the IR into a different language output. For example machine code.
        This phase often has to deal with instruction scheduling or register allocation while they have to output a fully working program.
\end{itemize}

\section{Interpreters}

There are different ways to interpret a language. This thesis implements a bytecode virtual machine (register-based) where the bytecode is the
intermidiate representation generated by the Nuua compiler. Among the most common interpreters built for programming languages, we may find the
following types of interpreters:

\begin{itemize}
    \item \textbf{Bytecode interpreter}: The programming language is first compiled into an intermediate bytecode representation and further executed
        by this virtual machine. A very famous example of a programming language interpreted by virtual machine is the Lua virtual machine \autocite{the_implementation_of_lua}
    \item \textbf{Abstract syntax tree interpreter}: This kind of interpreters just need the AST to work with, so no extra compilation to an intermediate
        representation is needed and therefore, easier to implement. However, due it's nature, they are much slower to execute and debug due the recursiveness
        of working with tree structures.
    \item \textbf{Just-in-time compiler}: Just-in-time compilers (often called JIT compilers) are an intermediate approach between a compiler that generates
    machine code and an interpreter. JIT compilers compile chunks of code at a specific moments while the program run to speed up portions of the code that is beeing interpreted (for example functions that are called frequently). Essentially, the JIT compiler needs to decide when to compile a specific part of the code at runtime and adds a small overhead in exchange for a machine-language performance on specific parts.
\end{itemize}

\section{System architecture}
\label{sec:sys_arch}

To design the system architecture of Nuua, consideration of existing system architectures needs to be taken since existing architectures
often work better than the custom-made ones and they often lead to greater project scalability. It's trivial
to make this choice before starting the project since changing a system architecture after it's initial
development phases becomes a very bad choice and may lead to a big ball of mud. Two choices in software development might be hierarchical
or layered systems. Nuua's architecture is based on a \emph{layered system} \autocite{software_architecture}.

A layered system has specific requirements regarding code communication. Specifically, a layered system consists
of different layers arranged vertically. Those layers have a specific criterion that needs to be met. As a matter of fact,
each layer can only use the layer below and gives an API for the layer above (if any) to use its functions. For example,
the \autoref{fig:layered_system} contains a simple 3-tier layered system. Layer 3 can only use Layer 2, and the output comes
from Layer 2. Layer 3 cannot use Layer 1 nor expect any outputs from it. It's the Layer 2 responsibility to use the Layer 1
and process its output before it can give its own output.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        [node distance=1.25cm]

        % Nodes of the layered system
        \node[block] (layerc) {Layer 3};
        \node[block,below=of layerc] (layerb) {Layer 2};
        \node[block,below=of layerb] (layera) {Layer 1};

        % Arrows going down
        \draw[line] ($(layerc.south) + (-1, 0)$) -- node[midway, left] {Uses} ($(layerb.north) + (-1, 0)$);
        \draw[line] ($(layerb.south) + (-1, 0)$) -- node[midway, left] {Uses} ($(layera.north) + (-1, 0)$);

        % Arrows going up
        \draw[line] ($(layerb.north) + (1, 0)$) -- node[midway, right] {Output} ($(layerc.south) + (1, 0)$);
        \draw[line] ($(layera.north) + (1, 0)$) -- node[midway, right] {Output} ($(layerb.south) + (1, 0)$);

    \end{tikzpicture}

    % Caption and Label
    \caption{Layered system}
    \label{fig:layered_system}
\end{figure}

This system is known to be robust and scalable at a very low-performance cost. It's very easy to understand and yet very powerful
for software systems like this one. These systems are also used for other complex software systems, such as operating
systems or complex protocols like TCP/IP.

By using a layered system each layer gets completely isolated and works independently by just using the layer below,
creating a very impressive way to scale-up or upgrade existing parts of the system without damaging the others.
However, a consistent API should, in fact, be established from the ground up to avoid breaking changes.
If the API is maintained, the individual layers may be upgraded independently without the need for extra work.

The Nuua architecture is shown at \autoref{fig:nuua_system}. An independent module called Logger is found
on the left side of the figure. This module is a logger used by all layers to output messages if needed (for example error reporting).

\begin{figure}[p]
    \centering
    \begin{tikzpicture}
        [node distance=1cm]

        % Nodes of the layered system
        \node[big_block] (app) {Application};
        \node[big_block,below=of app] (vm) {Virtual Machine};
        \node[big_block,below=of vm] (compiler) {Code generator};
        \node[big_block,below=of compiler] (analyzer) {Semantic analyzer};
        \node[big_block,below=of analyzer] (parser) {Parser};
        \node[big_block,below=of parser] (lexer) {Lexer};

        % Independent module
        \node[block, left=of compiler] (logger) {Logger};

        % Separator
        \draw[thick,dashed] ($(app.north west) + (-0.5, 0)$) -- ($(lexer.south west) + (-0.5, 0)$);

        % Top arrows
        \draw[line] (-3, 1.75) node[anchor=east] {Input} -| ($(app.north) + (-1, 0)$);
        \draw[line] ($(app.north) + (1, 0)$) |- (3, 1.75) node[anchor=west] {Output};

        % Arrows going down
        \draw[line] ($(app.south) + (-1, 0)$) -- ($(vm.north) + (-1, 0)$);
        \draw[line] ($(vm.south) + (-1, 0)$) -- ($(compiler.north) + (-1, 0)$);
        \draw[line] ($(compiler.south) + (-1, 0)$) -- ($(analyzer.north) + (-1, 0)$);
        \draw[line] ($(analyzer.south) + (-1, 0)$) -- ($(parser.north) + (-1, 0)$);
        \draw[line] ($(parser.south) + (-1, 0)$) -- ($(lexer.north) + (-1, 0)$);

        % Arrows going up
        \draw[line] ($(vm.north) + (1, 0)$) -- ($(app.south) + (1, 0)$);
        \draw[line] ($(compiler.north) + (1, 0)$) -- ($(vm.south) + (1, 0)$);
        \draw[line] ($(analyzer.north) + (1, 0)$) -- ($(compiler.south) + (1, 0)$);
        \draw[line] ($(parser.north) + (1, 0)$) -- ($(analyzer.south) + (1, 0)$);
        \draw[line] ($(lexer.north) + (1, 0)$) -- ($(parser.south) + (1, 0)$);

        % Logger arrows
        \draw[line] (app.west) -- (logger);
        \draw[line] (vm.west) -- (logger);
        \draw[line] (compiler.west) -- (logger);
        \draw[line] (analyzer.west) -- (logger);
        \draw[line] (parser.west) -- (logger);
        \draw[line] (lexer.west) -- (logger);

        % The right specifiers
        \draw[tuborg, decoration={brace}] let \p1=(app.north), \p2=(vm.south) in
            ($(2.5, \y1)$) -- ($(2.5, \y2)$) node[tubnode] {Interpreter};
        \draw[tuborg, decoration={brace}] let \p1=(compiler.north), \p2=(lexer.south) in
            ($(2.5, \y1)$) -- ($(2.5, \y2)$) node[tubnode] {Compiler};
    \end{tikzpicture}

    % Caption and Label
    \caption{Nuua's architecture (Layered System)}
    \label{fig:nuua_system}
\end{figure}

\begin{itemize}
    \item \textbf{Logger}: Used by all layers to debug or log errors. In case of a fatal error, the logger outputs
        an error stack in a fancy way and terminates the application.
    \item \textbf{Application}: This layer is used to decode the command line arguments and fire up the compiler toolchain. In short,
        the purpose is to analyze the command line arguments and fire the application accordingly.
    \item \textbf{Virtual Machine}: The virtual machine is the interpreter that runs Nuua. It's a register-based virtual machine that
        acts as the Nuua runtime environment.
    \item \textbf{Code generator}: Is responsible for the translation of the AST to the virtual machine bytecode. This acts as the code generation
        part of a compiler architecture.
    \item \textbf{Semantic analyzer}: Does all the semantic analysis of the compiler and optimizes the AST for faster and smaller output.
    \item \textbf{Parser}: Acts as the syntax analyzer, it uses a list of tokens and generates a fully valid AST.
    \item \textbf{Lexer}: Scans the source code and translates the characters to tokens, creating a list of tokens representing
        the original source code.
\end{itemize}
